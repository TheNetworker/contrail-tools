#!/usr/bin/env bash
#!/usr/bin/awk -f
TOOLS_WS=${TOOLS_WS:-$(pwd)}
multi_node=0

#install ant on cfgm0

function launch_testbed_virtual() {
    echo "Launch testbed using heat template"
    sshpass -p "c0ntrail123" ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -l root $BASE_CLUSTER " (
        set -e
        mkdir /root/${TEMPLATE_DIR}/
    ) "
    sshpass -p "c0ntrail123" scp -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ${TOOLS_WS}/templates/* root@$BASE_CLUSTER:/root/${TEMPLATE_DIR}/
    sshpass -p "c0ntrail123" ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -l root $BASE_CLUSTER " (
        set -e
        cp /root/${TEMPLATE_DIR}/$TEMPLATE /root/${TEMPLATE_DIR}/input.json
        cd /root/${TEMPLATE_DIR}/
        #Check if reimage param is set, otherwise use distro for launching VMs
        if [ -z "$REIMAGE_PARAM" ]
        then
            if [[ ( ${BRANCH} > "R3.2.99" ) || ( ${BRANCH} == "mainline" ) ]]
            then
                echo "build_infra_mainline"
                ./build_infra_mainline.sh ${TEMPLATE_DIR} ${DISTRO} ${BASE_CLUSTER} 
            else
                echo "build_infra"
                ./build_infra.sh ${TEMPLATE_DIR} ${DISTRO} ${BASE_CLUSTER}
            fi
        else
            if [[ ( ${BRANCH} > "R3.2.99" ) || ( ${BRANCH} == "mainline" ) ]]
            then
                echo "build_infra_mainline"
                ./build_infra_mainline.sh ${TEMPLATE_DIR} ${REIMAGE_PARAM} ${BASE_CLUSTER}
            else
                echo "build_infra"
                ./build_infra.sh ${TEMPLATE_DIR} ${REIMAGE_PARAM} ${BASE_CLUSTER}
            fi
        fi
        if [ $? == 0 ]
        then
            echo "Launch of Virtual testbed was successful!!!"
        else
            echo "Launch of virtual testbed failed, aborting"
            exit 1
        fi
    ) "
    sshpass -p "c0ntrail123" scp -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null root@$BASE_CLUSTER:/root/${TEMPLATE_DIR}/server-manager-file ${TOOLS_WS}/
    export SM_SERVER_IP=`cat ${TOOLS_WS}/server-manager-file`
    sshpass -p "c0ntrail123" scp -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null root@$BASE_CLUSTER:/root/${TEMPLATE_DIR}/config-node-ip ${TOOLS_WS}/
    sshpass -p "c0ntrail123" scp -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null root@$BASE_CLUSTER:/root/${TEMPLATE_DIR}/info.txt ${TOOLS_WS}/
    sshpass -p "c0ntrail123" scp -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null root@$BASE_CLUSTER:/root/${TEMPLATE_DIR}/sm-control-data-ip ${TOOLS_WS}/
    #Copy testbed_file generated by build_infra.sh
    sshpass -p "c0ntrail123" scp -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null root@$BASE_CLUSTER:/root/${TEMPLATE_DIR}/testbed.py ${TOOLS_WS}/testbeds/$TBFILE
}

function delete_stacks() {
    sshpass -p "c0ntrail123" ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -l root $BASE_CLUSTER " (
        set -e
        cd /root/${TEMPLATE_DIR}/
        ./delete_stack.sh info.txt 
    )"
}

function update_byo_yml() {
    #Function to get node IPs from testbed.py
    #sed into the ose-install template
}

function run_openshift_ansible() {
    export OPENSHIFT_ANSIBLE=$TOOLS_WS/openshift-ansible
    update_ose-install || debug_and_die "Failed to update byo yml"
    cd $OPENSHIFT_ANSIBLE
    ansible-playbook -i inventory/byo/ose-install playbooks/byo/config.yml
}

function copy_contrail_ansible() {
    export CONTRAIL_ANSIBLE=$TOOLS_WS/contrail-ansible
    mkdir $CONTRAIL_ANSIBLE && cd $CONTRAIL_ANSIBLE
    wget http://10.84.5.120/github-build/R4.0/20/ubuntu-14-04/mitaka/artifacts_extra/contrail-ansible-4.0.0.0-20.tar.gz
    tar -xvzf contrail-ansible-4.0.0.0-20.tar.gz
    mkdir playbooks/container_images && cd playbooks/container_images
    wget http://10.84.5.120/github-build/R4.0/20/ubuntu-14-04/mitaka/artifacts/contrail-kubernetes-docker-images_4.0.0.0-20.tgz
    
    
}
function run_contrail_ansible() {
    update_all_yml
    update hosts_file
    cd $CONTRAIL_ANSIBLE
    ansible-playbook -i inventory/my-inventory site.yml   
}

#need to run it from ansible node
function enable_passwordless_ssh() {
    python tools/setup_ssh_keys.py ${TOOLS_WS}/testbeds/${tb_filename}
}

function run_openshift_virtual_task() {
    export RUN_SUCCESSFUL=0
    echo "Running tests on $1.."
    #Change testbed filename to template_HA.py
    export TEMPLATE_DIR=${SCRIPT_TIMESTAMP}
    export TBFILE="template_HA.py"    
    export TBFILE_NAME="template_HA.py"
    launch_testbed_virtual || debug_and_die "Failed to launch testbed setup"
    echo $SM_SERVER_IP
    if [ ${TEST_SETUP} == "SINGLENODE" ]
    then
        multi_node=0
    elif [ ${TEST_SETUP} == "MULTINODE" ]
    then
        multi_node=1
    elif [ ${TEST_SETUP} == "MULTIINTERFACE" ]
    then
        multi_node=1
    else
        echo "TEST_SETUP is not defined, abort the process"
        exit 1
    fi
    if [ -z "$SM_SERVER_IP" ]
    then
        echo "SM_SERVER_IP is not set, unable to proceede, aborting the process"
        exit 1
    fi

    if [ -z $CLUSTER_NAME ]; then
        echo "CLUSTER_NAME env is not set, aborting the process"
        exit 1
    fi
    create_testbed || die "Failed to create required testbed details"
    enable_passwordless_ssh || die "Unable to enable password less ssh"
    run_openshift_ansible || die "Openshift provision failed"
    copy_contrail_ansible
    run_contrail_ansible
    
    #Add server and cluster to the SM

    #check_kernel_upgrade || die "kernel upgrade failed"
    export API_SERVER_HOST_STRING="root@"`cat ${TOOLS_WS}/config-node-ip`
    
    if [[ $TEST_RUN_INFRA == 'docker' ]]; then
        search_package
        pkg_file_name=`basename $PKG_FILE`
        export PACKAGE_VERSION=`echo ${pkg_file_name} | sed 's/contrail-cloud-docker[-_]\([0-9\.\-]*\)-.*/\1/'`
        if [[ -z $TEST_HOST_STRING ]]; then
            export TEST_HOST_STRING=$API_SERVER_HOST_STRING
            export TEST_HOST_PASSWORD=$API_SERVER_HOST_PASSWORD
        fi
        export TEST_HOST_IP=`echo $TEST_HOST_STRING | cut -d @ -f2`
        export TEST_HOST_USER=`echo $TEST_HOST_STRING | cut -d @ -f1`
        #Copy testbed_file generated by build_infra.sh
        sshpass -p "c0ntrail123" scp -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null root@$BASE_CLUSTER:/root/${TEMPLATE_DIR}/testbed.py ${TOOLS_WS}/testbeds/${tb_filename}
        setup_testnode || debug_and_die "test node setup failed"
        install_dep_pkgs_for_test
        run_sanity_simple || debug_and_die "run_sanity_simple failed"
    else
        add_sourcelist || die "source.list copy failed on all target nodes"
        install_third_party_pkgs || die "installing GDB/ant failed"
        install_dep_pkgs_for_test
        run_sanity || debug_and_die "Run_sanity step failed"
    fi

    run_tempest || die "Run_Tempest step failed"
    collect_tech_support || die "Task to collect logs/cores failed"
    #delete_stacks || die "Failed to delete stacks"
    echo "Ending test on "${AVAILABLE_TESTBEDS}
    #Set the TBFILE_NAME to the one passed through jenkins job
    export RUN_SUCCESSFUL=1
    TBFILE_NAME=${AVAILABLE_TESTBEDS}
    tb_lock_file=${LOCK_FILE_DIR}/${TBFILE_NAME}
}


function debug_and_die
{
    local message=$1
    #Set the TBFILE_NAME to the one passed through jenkins job
    TBFILE_NAME=${AVAILABLE_TESTBEDS}
    tb_lock_file=${LOCK_FILE_DIR}/${TBFILE_NAME}
    if [ $LOCK_TESTBED_ON_FAILURE -eq 1 ]; then
        echo "Testbed is set to be locked on failure and add stack info to lock file"
        #Store stack info in testbed lock file
        cat ${TOOLS_WS}/info.txt >> ${tb_lock_file} 
        if [[ $message =~ 'Test failures exceed' ]]; then
            collect_tech_support
        fi
        export RELEASE_TESTBED=0
        lockfile ${LOCK_FILE_DIR}/lockfile1

        set -x
        echo "Locking testbed $tb_lock_file for debugging"
        echo "Testbed locked..Unlock when debug complete" >> $tb_lock_file
        cat $tb_lock_file

        remove_lock_file
    else
        collect_tech_support
        echo "Testbed stack delete!!!"
        delete_stacks || die "Failed to delete stacks"
        if [[ $VCENTER_ONLY_TESTBED -eq 1  || $VCENTER_AS_COMPUTE_TESTBED -eq 1 ]]; then
            # deregister the setup from vcenter server
            run_fab cleanup_vcenter
        fi
    fi
    [ -z "$message" ] && message="Died"
    echo "${BASH_SOURCE[1]}: line ${BASH_LINENO[0]}: ${FUNCNAME[1]}: $message." >&2
    cat $tb_lock_file
    python ${TOOLS_WS}/testers/upload.py --pkg_name $PKG_FILE --jenkins_id $SCRIPT_TIMESTAMP
    exit 1
}

function cleanup() {
    #Set the TBFILE_NAME to the one passed through jenkins job
    TBFILE_NAME=${AVAILABLE_TESTBEDS}
    tb_lock_file=${LOCK_FILE_DIR}/${TBFILE_NAME}
    if [ $LOCK_TESTBED_ON_FAILURE -eq 0 ] && [ $RUN_SUCCESSFUL -eq 1 ]; then
        delete_stacks || die "Failed to delete stacks"
    else
        #Store stack info in testbed lock file
        cat ${TOOLS_WS}/info.txt >> ${tb_lock_file} 
    fi
    unlock_testbed $TBFILE_NAME || die "Failed to unlock testbed $TBFILE_NAME"
}
